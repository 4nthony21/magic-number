# Magic-Number
Challenge Data Engineer

## Summary
This repository contains a simple ETL pipeline that downloads public ZIP files, extracts CSVs, and loads them into a SQLite database (`Annies.db`). It is intended as a sample solution for processing transactional data sets.

## Repository structure
- `ETL/`: extraction and loading scripts (`config.py`, `extract.py`, `load.py`).
- `Data/`: local location for downloaded CSV files (generated by `extract.py`).
- `Analyze/`: queries and analysis (e.g., `queries.sql`).
- `Annies.db`: SQLite database (created when running `load.py`).
- `requirements.txt`: Python dependencies.

## Requirements
- Python 3.9+ recommended
- Packages (see `requirements.txt`): `pandas`, `requests`

## Quick installation
1. Create and activate a virtual environment (optional but recommended):

```bash
python -m venv venv
# Windows PowerShell
venv\Scripts\Activate.ps1
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

## Usage
1. Run extraction and loading (from the root of the repo):

```bash
python ETL/load.py
```

This runs `ETL/extract.py` (downloads and extracts the CSV files in `Data/`) and then loads the CSV files listed in `ETL/config.py` into `Annies.db`.

## ETL Details
- `ETL/config.py`: lists the source URLs, table names, and local paths.
- `ETL/extract.py`: downloads the ZIP files, extracts the CSV, and renames the file according to `config.URLS`.
- `ETL/load.py`: runs `extract.py` and loads the CSV files into SQLite with `pandas.DataFrame.to_sql`.

## Inspection Tools

A utility script is included to quickly inspect the generated database `Annies.db`:

```bash
python scripts/inspect_db.py --db Annies.db --samples 5
```

The script lists the tables, shows the row count per table, and presents up to `--samples` sample rows per table.

## Tests and CI

Test suites based on `pytest` are included in the `tests/` folder. The main test `tests/test_validate_schemas.py` validates that the CSVs to be loaded contain the expected columns (uses `ETL/schemas.py`).

To run locally:

```bash
pip install -r requirements.txt
# extract CSVs before running tests (if necessary):
python ETL/extract.py
pytest -q
```

The CI pipeline (`.github/workflows/ci.yml`) runs `pytest` first; then it uses `Annies.db` included in the repo if it exists, and only if that fails does it run `ETL/load.py` (avoiding unnecessary downloads in CI).

## Recent changes

- `ETL/extract.py`: migrated to an implementation with `requests.Session` that includes retries and backoff; now logs activity and errors with `logging`.
- `ETL/schemas.py`: map `EXPECTED_COLUMNS` with minimum expected columns per table.
- Tests: added `tests/test_validate_schemas.py` which validates CSV headers before loading.
- CI: added workflow `.github/workflows/ci.yml` which runs `pytest`, uses included `Annies.db` if available, and runs `scripts/inspect_db.py` for final checks.
- Utilities: `scripts/inspect_db.py` displays tables, counts, and sample rows.

These changes improve the robustness of the pipeline and allow the integrity of CSVs to be validated before loading into the database.

## Deployment on AWS (quick guide)

Summary: It is recommended to containerize the application and deploy it on AWS using ECR + ECS (Fargate) or a scheduler (EventBridge) to periodically run the pipeline. The database in production should be `Postgres` (RDS). Use `Secrets Manager` for credentials and `CloudWatch` for logs.

Minimum steps (summary):

1. Prepare `Dockerfile` in the root of the repo (minimum example):

```dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
ENV PYTHONUNBUFFERED=1
CMD [“python”,“ETL/load.py”]
```

2. Create a repository in ECR and upload the image:

```bash
# create repository (once)
aws ecr create-repository --repository-name magic-number --region us-east-1

# log in and push
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <AWS_ACCOUNT_ID>.dkr.ecr.us-east-1.amazonaws.com
docker build -t magic-number:latest .
docker tag magic-number:latest <AWS_ACCOUNT_ID>.dkr.ecr.us-east-1.amazonaws.com/magic-number:latest
docker push <AWS_ACCOUNT_ID>.dkr.ecr.us-east-1.amazonaws.com/magic-number:latest
```

3. Prepare database (RDS Postgres) or use a managed service. Write down endpoint and credentials.

4. Save credentials in Secrets Manager (e.g., `magic-number/db`), for example, a JSON with `DATABASE_URL`:

```bash
aws secretsmanager create-secret --name magic-number/db --secret-string ‘{“DATABASE_URL”:“postgres://user:pass@host:5432/dbname”}’
```

5. Create an ECS Fargate cluster and a Task Definition that uses the ECR image. Configure environment variables in the task: `DATABASE_URL`, `LOCAL_PATH` (if using S3, configure credentials and bucket), and enable CloudWatch Logs.

6. Schedule: create an EventBridge rule (cron) that invokes `RunTask` in ECS to execute the Task Definition at the desired time (e.g., daily). Alternative: use AWS Batch or Step Functions if the pipeline grows.

7. Observability and permissions:
- Configure the Task IAM Role with minimal permissions to access Secrets Manager, S3 (if used), and CloudWatch Logs.
- Ensure automatic backups for RDS and configure CloudWatch alerts for errors/high times.

Notes and recommendations:
- Parameterize `ETL/load.py` to read `DATABASE_URL` from `os.environ` instead of using local `Annies.db`.
- Consider uploading/reading data from S3 instead of mounting `Data/` in the container. This allows for scaling and persistence.
- For rapid deployments, it is also possible to use AWS Lambda with container image (if the execution fits within Lambda limits).

